"""
Medical Safety Module
Handles medical AI safety, hallucination detection, and compliance
"""

import asyncio
import re
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from enum import Enum
import json
from datetime import datetime

from ..core.logging import LoggerMixin
from ..core.base_exceptions import ComponentExecutionException

class RiskLevel(Enum):
    """Medical information risk levels"""
    LOW = "low"           # General health information
    MEDIUM = "medium"     # Specific symptoms/conditions
    HIGH = "high"         # Treatment recommendations
    CRITICAL = "critical" # Emergency situations

class VerificationStatus(Enum):
    """Verification status for medical claims"""
    VERIFIED = "verified"
    UNCERTAIN = "uncertain"
    CONTRADICTED = "contradicted"
    INSUFFICIENT_DATA = "insufficient_data"

@dataclass
class MedicalClaim:
    """Medical claim for verification"""
    text: str
    category: str  # symptom, treatment, diagnosis, etc.
    risk_level: RiskLevel
    confidence_score: float = 0.0
    sources: List[str] = None
    verification_status: VerificationStatus = VerificationStatus.INSUFFICIENT_DATA

@dataclass
class SafetyCheckResult:
    """Result of medical safety check"""
    is_safe: bool
    risk_level: RiskLevel
    issues_found: List[str]
    recommendations: List[str]
    requires_disclaimer: bool
    blocked_content: List[str]
    confidence_score: float

class MedicalSafetyChecker(LoggerMixin):
    """Medical AI safety checker and hallucination detector"""
    
    def __init__(self):
        super().__init__()
        self.setup_safety_rules()
        self.setup_medical_knowledge_base()
    
    def setup_safety_rules(self):
        """Setup medical safety rules"""
        
        # Prohibited content patterns
        self.prohibited_patterns = [
            r"è¯Šæ–­ä¸º|ç¡®è¯Šä¸º|æˆ‘è¯Šæ–­ä½ ",  # Diagnosis claims
            r"å»ºè®®ç«‹å³æœç”¨|æ¨èè¯ç‰©|æœç”¨.*æ¯«å…‹",  # Medication recommendations
            r"ä¸éœ€è¦çœ‹åŒ»ç”Ÿ|æ— éœ€å°±åŒ»",  # Discouraging medical consultation
            r"è¿™æ˜¯ç™Œç—‡|ä½ æ‚£æœ‰.*ç—…",  # Direct disease attribution
            r"ç«‹å³æ‰‹æœ¯|éœ€è¦æ‰‹æœ¯",  # Surgical recommendations
        ]
        
        # High-risk keywords requiring extra caution
        self.high_risk_keywords = {
            'emergency': ['å¿ƒè„ç—…', 'ä¸­é£', 'æ˜è¿·', 'å¤§å‡ºè¡€', 'å‘¼å¸å›°éš¾', 'èƒ¸ç—›'],
            'diagnosis': ['ç™Œç—‡', 'è‚¿ç˜¤', 'ç³–å°¿ç—…', 'é«˜è¡€å‹', 'æŠ‘éƒç—‡'],
            'medication': ['æŠ—ç”Ÿç´ ', 'èƒ°å²›ç´ ', 'æ¿€ç´ ', 'åŒ–ç–—', 'æ‰‹æœ¯'],
            'children': ['å©´å„¿', 'å„¿ç«¥', 'æ–°ç”Ÿå„¿', 'æ€€å­•', 'å­•å¦‡']
        }
        
        # Required disclaimers by risk level
        self.disclaimers = {
            RiskLevel.LOW: "æ­¤ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚",
            RiskLevel.MEDIUM: "âš ï¸ æ­¤ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå¦‚æœ‰ç—‡çŠ¶è¯·å’¨è¯¢åŒ»ç–—ä¸“ä¸šäººå‘˜ã€‚",
            RiskLevel.HIGH: "âš ï¸ é‡è¦ï¼šæ­¤ä¿¡æ¯ä¸èƒ½æ›¿ä»£åŒ»ç–—è¯Šæ–­ï¼Œè¯·ç«‹å³å’¨è¯¢åˆæ ¼åŒ»ç”Ÿã€‚",
            RiskLevel.CRITICAL: "ğŸš¨ ç´§æ€¥æé†’ï¼šå¦‚é‡ç´§æ€¥æƒ…å†µè¯·ç«‹å³å°±åŒ»æˆ–æ‹¨æ‰“æ€¥æ•‘ç”µè¯ï¼"
        }
    
    def setup_medical_knowledge_base(self):
        """Setup medical knowledge base for verification"""
        # This would connect to authoritative medical databases
        self.knowledge_sources = {
            "symptom_database": "åŒ»å­¦ç—‡çŠ¶æ•°æ®åº“",
            "drug_database": "è¯ç‰©å®‰å…¨æ•°æ®åº“", 
            "treatment_guidelines": "åŒ»ç–—æŒ‡å—æ•°æ®åº“",
            "medical_literature": "åŒ»å­¦æ–‡çŒ®æ•°æ®åº“"
        }
        
        # Common medical fact patterns for verification
        self.fact_patterns = {
            "vital_signs": {
                "è¡€å‹æ­£å¸¸èŒƒå›´": {"systolic": (90, 140), "diastolic": (60, 90)},
                "æ­£å¸¸ä½“æ¸©": (36.1, 37.2),
                "æ­£å¸¸å¿ƒç‡": (60, 100)
            },
            "emergency_signs": [
                "èƒ¸ç—›", "å‘¼å¸å›°éš¾", "æ˜è¿·", "å¤§é‡å‡ºè¡€", "æŒç»­é«˜çƒ§"
            ]
        }
    
    async def check_response_safety(self, 
                                  response: str, 
                                  query: str = "",
                                  context: Dict[str, Any] = None) -> SafetyCheckResult:
        """Comprehensive safety check for medical response"""
        
        issues = []
        recommendations = []
        blocked_content = []
        risk_level = RiskLevel.LOW
        
        # 1. Check for prohibited patterns
        prohibited_issues = self._check_prohibited_patterns(response)
        if prohibited_issues:
            issues.extend(prohibited_issues)
            blocked_content.extend(prohibited_issues)
            risk_level = RiskLevel.HIGH
        
        # 2. Assess risk level based on content
        content_risk = self._assess_content_risk(response)
        risk_level = max(risk_level, content_risk)
        
        # 3. Check for medical claims that need verification
        unverified_claims = await self._check_medical_claims(response)
        if unverified_claims:
            issues.append(f"åŒ…å« {len(unverified_claims)} ä¸ªæœªéªŒè¯çš„åŒ»å­¦å£°æ˜")
            recommendations.append("å»ºè®®éªŒè¯åŒ»å­¦å£°æ˜çš„å‡†ç¡®æ€§")
        
        # 4. Check for hallucination indicators
        hallucination_risk = await self._detect_hallucination_risk(response, query)
        if hallucination_risk > 0.5:
            issues.append("æ£€æµ‹åˆ°å¯èƒ½çš„AIå¹»è§‰å†…å®¹")
            recommendations.append("éœ€è¦é¢å¤–éªŒè¯å†…å®¹å‡†ç¡®æ€§")
            risk_level = max(risk_level, RiskLevel.MEDIUM)
        
        # 5. Check completeness and accuracy
        completeness_score = self._check_response_completeness(response, query)
        
        # 6. Final safety assessment
        is_safe = (len(blocked_content) == 0 and 
                  risk_level != RiskLevel.CRITICAL and
                  hallucination_risk < 0.7)
        
        # Add risk-appropriate recommendations
        if risk_level >= RiskLevel.MEDIUM:
            recommendations.append("æ·»åŠ åŒ»ç–—å…è´£å£°æ˜")
        
        if risk_level >= RiskLevel.HIGH:
            recommendations.append("å»ºè®®ç”¨æˆ·å’¨è¯¢åŒ»ç–—ä¸“ä¸šäººå‘˜")
        
        confidence_score = max(0.0, 1.0 - hallucination_risk - (len(unverified_claims) * 0.1))
        
        return SafetyCheckResult(
            is_safe=is_safe,
            risk_level=risk_level,
            issues_found=issues,
            recommendations=recommendations,
            requires_disclaimer=risk_level >= RiskLevel.MEDIUM,
            blocked_content=blocked_content,
            confidence_score=confidence_score
        )
    
    def _check_prohibited_patterns(self, text: str) -> List[str]:
        """Check for prohibited medical content patterns"""
        violations = []
        
        for pattern in self.prohibited_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                violations.append(f"åŒ…å«ç¦æ­¢çš„åŒ»ç–—å£°æ˜: {matches[0]}")
        
        return violations
    
    def _assess_content_risk(self, text: str) -> RiskLevel:
        """Assess risk level based on content analysis"""
        text_lower = text.lower()
        
        # Check for emergency keywords
        if any(keyword in text_lower for keyword in self.high_risk_keywords['emergency']):
            return RiskLevel.CRITICAL
        
        # Check for diagnostic content
        if any(keyword in text_lower for keyword in self.high_risk_keywords['diagnosis']):
            return RiskLevel.HIGH
        
        # Check for medication content
        if any(keyword in text_lower for keyword in self.high_risk_keywords['medication']):
            return RiskLevel.HIGH
        
        # Check for children/pregnancy content
        if any(keyword in text_lower for keyword in self.high_risk_keywords['children']):
            return RiskLevel.MEDIUM
        
        return RiskLevel.LOW
    
    async def _check_medical_claims(self, text: str) -> List[MedicalClaim]:
        """Extract and verify medical claims"""
        claims = []
        
        # Extract potential medical claims using patterns
        claim_patterns = [
            r"æ­£å¸¸.*èŒƒå›´.*[0-9]+",  # Normal ranges
            r"ç—‡çŠ¶åŒ…æ‹¬.*",          # Symptoms
            r"æ²»ç–—æ–¹æ³•.*",          # Treatments
            r".*ä¼šå¯¼è‡´.*"           # Causation claims
        ]
        
        for pattern in claim_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                claim = MedicalClaim(
                    text=match,
                    category="general",
                    risk_level=self._assess_claim_risk(match)
                )
                
                # Attempt to verify claim
                claim.verification_status = await self._verify_medical_claim(claim)
                claims.append(claim)
        
        return [claim for claim in claims 
                if claim.verification_status in [VerificationStatus.UNCERTAIN, 
                                               VerificationStatus.CONTRADICTED]]
    
    async def _verify_medical_claim(self, claim: MedicalClaim) -> VerificationStatus:
        """Verify medical claim against knowledge base"""
        # This would implement actual verification against medical databases
        
        # For now, implement basic verification logic
        claim_text = claim.text.lower()
        
        # Check vital signs claims
        if "è¡€å‹" in claim_text and "æ­£å¸¸" in claim_text:
            # Extract numbers and check against known ranges
            numbers = re.findall(r'\d+', claim.text)
            if len(numbers) >= 2:
                systolic, diastolic = int(numbers[0]), int(numbers[1])
                normal_range = self.fact_patterns["vital_signs"]["è¡€å‹æ­£å¸¸èŒƒå›´"]
                
                if (normal_range["systolic"][0] <= systolic <= normal_range["systolic"][1] and
                    normal_range["diastolic"][0] <= diastolic <= normal_range["diastolic"][1]):
                    return VerificationStatus.VERIFIED
                else:
                    return VerificationStatus.CONTRADICTED
        
        # Default to uncertain for complex claims
        return VerificationStatus.UNCERTAIN
    
    def _assess_claim_risk(self, claim_text: str) -> RiskLevel:
        """Assess risk level of a medical claim"""
        if any(keyword in claim_text.lower() for keyword in self.high_risk_keywords['emergency']):
            return RiskLevel.CRITICAL
        elif any(keyword in claim_text.lower() for keyword in self.high_risk_keywords['diagnosis']):
            return RiskLevel.HIGH
        elif any(keyword in claim_text.lower() for keyword in self.high_risk_keywords['medication']):
            return RiskLevel.HIGH
        else:
            return RiskLevel.MEDIUM
    
    async def _detect_hallucination_risk(self, response: str, query: str) -> float:
        """Detect potential AI hallucination in medical context"""
        risk_score = 0.0
        
        # Check for overly specific medical claims without qualification
        specific_patterns = [
            r'\d+%çš„.*æ‚£è€…',      # Specific percentages
            r'ç ”ç©¶è¡¨æ˜.*\d+',     # Studies with specific numbers
            r'åŒ»ç”Ÿå»ºè®®.*',        # Doctor recommendations
            r'æ ¹æ®.*åŒ»é™¢.*'       # Hospital references
        ]
        
        for pattern in specific_patterns:
            if re.search(pattern, response):
                risk_score += 0.2
        
        # Check for contradictory information within response
        if self._contains_contradictions(response):
            risk_score += 0.3
        
        # Check for unrealistic claims
        unrealistic_patterns = [
            r'100%.*æ²»æ„ˆ',      # 100% cure claims
            r'ç»å¯¹.*å®‰å…¨',      # Absolute safety claims
            r'æ°¸è¿œä¸ä¼š.*',      # Never claims
            r'ç«‹å³.*æ²»å¥½'       # Immediate cure claims
        ]
        
        for pattern in unrealistic_patterns:
            if re.search(pattern, response, re.IGNORECASE):
                risk_score += 0.4
        
        # Normalize score
        return min(risk_score, 1.0)
    
    def _contains_contradictions(self, text: str) -> bool:
        """Simple contradiction detection"""
        # Look for contradictory statements
        contradictory_pairs = [
            (['å®‰å…¨', 'æ— å®³'], ['å±é™©', 'æœ‰å®³']),
            (['æ¨è', 'å»ºè®®'], ['ä¸æ¨è', 'ä¸å»ºè®®']),
            (['æ­£å¸¸', 'å¥åº·'], ['å¼‚å¸¸', 'ä¸å¥åº·'])
        ]
        
        text_lower = text.lower()
        for positive, negative in contradictory_pairs:
            has_positive = any(word in text_lower for word in positive)
            has_negative = any(word in text_lower for word in negative)
            
            if has_positive and has_negative:
                return True
        
        return False
    
    def _check_response_completeness(self, response: str, query: str) -> float:
        """Check if response adequately addresses the query"""
        # Simple completeness scoring
        query_keywords = set(re.findall(r'\w+', query.lower()))
        response_keywords = set(re.findall(r'\w+', response.lower()))
        
        overlap = len(query_keywords & response_keywords)
        total_query_keywords = len(query_keywords)
        
        if total_query_keywords == 0:
            return 1.0
        
        return overlap / total_query_keywords
    
    async def apply_safety_measures(self, 
                                   response: str, 
                                   safety_result: SafetyCheckResult) -> str:
        """Apply safety measures to response"""
        
        if not safety_result.is_safe:
            # Block unsafe content
            if safety_result.blocked_content:
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æä¾›å¯èƒ½ä¸å®‰å…¨çš„åŒ»ç–—å»ºè®®ã€‚è¯·å’¨è¯¢åˆæ ¼çš„åŒ»ç–—ä¸“ä¸šäººå‘˜ã€‚"
        
        modified_response = response
        
        # Add disclaimer based on risk level
        if safety_result.requires_disclaimer:
            disclaimer = self.disclaimers[safety_result.risk_level]
            modified_response = f"{response}\n\n{disclaimer}"
        
        # Add verification notice for uncertain claims
        if safety_result.confidence_score < 0.7:
            verification_notice = "\n\nğŸ“‹ è¯·æ³¨æ„ï¼šä¸Šè¿°ä¿¡æ¯çš„å‡†ç¡®æ€§éœ€è¦è¿›ä¸€æ­¥éªŒè¯ï¼Œå»ºè®®å’¨è¯¢åŒ»ç–—ä¸“ä¸šäººå‘˜ã€‚"
            modified_response += verification_notice
        
        # Add emergency notice for high-risk content
        if safety_result.risk_level == RiskLevel.CRITICAL:
            emergency_notice = "\n\nğŸš¨ å¦‚æœæ‚¨æ­£åœ¨ç»å†åŒ»ç–—ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“æ€¥æ•‘ç”µè¯æˆ–å‰å¾€æœ€è¿‘çš„åŒ»é™¢ï¼"
            modified_response += emergency_notice
        
        return modified_response
    
    async def generate_safety_report(self, 
                                   query: str, 
                                   response: str, 
                                   safety_result: SafetyCheckResult) -> Dict[str, Any]:
        """Generate detailed safety report for logging"""
        
        return {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "response_length": len(response),
            "safety_assessment": {
                "is_safe": safety_result.is_safe,
                "risk_level": safety_result.risk_level.value,
                "confidence_score": safety_result.confidence_score,
                "issues_count": len(safety_result.issues_found),
                "requires_disclaimer": safety_result.requires_disclaimer
            },
            "issues_found": safety_result.issues_found,
            "recommendations": safety_result.recommendations,
            "blocked_content": safety_result.blocked_content,
            "verification_status": "pending_expert_review" if safety_result.confidence_score < 0.5 else "auto_approved"
        }

class MedicalFactVerifier(LoggerMixin):
    """Dedicated fact verification for medical claims"""
    
    def __init__(self):
        super().__init__()
        self.authoritative_sources = {
            "who": "World Health Organization",
            "cdc": "Centers for Disease Control", 
            "mayo": "Mayo Clinic",
            "pubmed": "PubMed Medical Literature"
        }
    
    async def verify_medical_fact(self, 
                                 claim: str, 
                                 sources: List[str] = None) -> Dict[str, Any]:
        """Verify medical fact against authoritative sources"""
        
        verification_result = {
            "claim": claim,
            "verified": False,
            "confidence": 0.0,
            "sources_checked": [],
            "supporting_evidence": [],
            "contradicting_evidence": [],
            "recommendation": "expert_verification_required"
        }
        
        # This would implement actual fact-checking against medical databases
        # For now, implement basic pattern matching
        
        if self._is_basic_fact(claim):
            verification_result.update({
                "verified": True,
                "confidence": 0.8,
                "recommendation": "likely_accurate"
            })
        
        return verification_result
    
    def _is_basic_fact(self, claim: str) -> bool:
        """Check if claim is a basic medical fact"""
        basic_facts = [
            "æ­£å¸¸ä½“æ¸©", "è¡€å‹èŒƒå›´", "å¿ƒç‡æ­£å¸¸", "å¤šå–æ°´", "å……è¶³ç¡çœ ", "å‡è¡¡é¥®é£Ÿ"
        ]
        
        return any(fact in claim for fact in basic_facts)